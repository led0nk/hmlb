---
apiVersion: v1
kind: Namespace
metadata:
  name: observability
labels:
  pod-security.kubernetes.io/audit: privileged
  pod-security.kubernetes.io/enforce: privileged
  pod-security.kubernetes.io/warn: privileged
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: open-telemetry
  namespace: observability
spec:
url: https://open-telemetry.github.io/opentelemetry-helm-charts
interval: 1h
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: opentelemetry-operator
  namespace: observability
spec:
  interval: 1h
  chart:
    spec:
      chart: opentelemetry-operator
      sourceRef:
        kind: HelmRepository
        name: open-telemetry
        namespace: observability
      version: "0.79.0"
  install:
    createNamespace: true
  values:
    manager:
      collectorImage:
        repository: otel/opentelemetry-collector-k8s
    admissionWebhooks:
      certManager:
        enabled: true
      autoGenerateCert:
        enabled: true
---
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel-host-agent
  namespace: observability
spec:
  serviceAccount: otel-hostfs-daemonset
  mode: daemonset
  volumeMounts:
    - mountPath: /hostfs
      name: hostfs
      readOnly: true
  volumes:
    - name: hostfs
      hostPath:
        path: /
        type: Directory
  env:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
  config: 
    receivers:
      hostmetrics:
        root_path: /hostfs
        resource_to_telemetry_conversion:
          enabled: true
        collection_interval: 30s
        scrapers:
          cpu:
            metrics:
              system.cpu.utilization:
                enabled: true
              system.cpu.logical.count:
                enabled: true
          memory:
            metrics:
              system.memory.utilization:
                enabled: true
              system.memory.limit:
                enabled: true
          disk: {}
          filesystem: {}
          network: {}
          processes: {}
          process:
            mute_process_user_error: true
            mute_process_exe_error: true
            metrics:
              process.cpu.utilization:
                enabled: true
              process.memory.utilization:
                enabled: true
      kubeletstats:
        collection_interval: 30s
        auth_type: "serviceAccount"
        endpoint: "https://${env:K8S_NODE_NAME}:10250"
        insecure_skip_verify: true
        node: "${env:K8S_NODE_NAME}"
        k8s_api_config:
          auth_type: serviceAccount
        metrics:
          k8s.node.uptime:
            enabled: true
          k8s.pod.uptime:
            enabled: true
          k8s.pod.cpu.node.utilization:
            enabled: true
          k8s.pod.cpu_limit_utilization:
            enabled: true
          k8s.pod.cpu_request_utilization:
            enabled: true
          k8s.pod.memory.node.utilization:
            enabled: true
          k8s.pod.memory_limit_utilization:
            enabled: true
          k8s.pod.memory_request_utilization:
            enabled: true

    processors:
      resourcedetection/system:
        detectors: ["system"]
        system:
          hostname_sources: ["os"]
      resource:
        attributes:
          - action: upsert
            key: host.name
            value: "${env:K8S_NODE_NAME}"
      transform:
        metric_statements:
          - context: datapoint
            statements:
              - set(attributes["host.name"], resource.attributes["host.name"])
              - set(attributes["process.command"], resource.attributes["process.command"])
              - set(attributes["process.command_line"], resource.attributes["process.command_line"])
              - set(attributes["process.executable.name"], resource.attributes["process.executable.name"])
              - set(attributes["process.executable.path"], resource.attributes["process.executable.path"])
              - set(attributes["process.owner"], resource.attributes["process.owner"])
              - set(attributes["process.parent_pid"], resource.attributes["process.parent_pid"])
              - set(attributes["process.pid"], resource.attributes["process.pid"])
      batch: {}

    exporters:
      otlp:
        endpoint: otel-lgtm.observability.svc.cluster.local:4317
        tls:
          insecure: true

    service:
      pipelines:
        metrics:
          receivers: [hostmetrics, kubeletstats]
          processors: [resourcedetection/system, resource, transform, batch]
          exporters: [otlp]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-hostfs-daemonset
  namespace: observability
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-kubelet
rules:
  - apiGroups: ['']
    resources: ['nodes/stats']
    verbs: ['get', 'watch', 'list']
  - apiGroups: [""]
    resources: ["nodes/proxy"]
    verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-kubelet-binding
subjects:
  - kind: ServiceAccount
    name: otel-hostfs-daemonset
    namespace: observability
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-kubelet
---
##################### 
# OTEL-LGTM-STACK
#####################
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: lgtm-observability
  namespace: observability
spec:
  storageClassName: local-path
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-lgtm
  namespace: observability
  labels:
    app: otel-lgtm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-lgtm
  template:
    metadata:
      labels:
        app: otel-lgtm
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - worker
            - weight: 50
              preference:
                matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - master
      containers:
        - name: otel-lgtm
          image: grafana/otel-lgtm:latest
          env:
            - name: ENABLE_LOGS_OTELCOL
              value: "true"
            - name: LGTM_APP_CONFIG_FILE
              value: /etc/lgtm/lgtm-config.yaml
          ports:
            - containerPort: 3000
            - containerPort: 3100
            - containerPort: 4317
            - containerPort: 4318
            - containerPort: 14268
          volumeMounts:
            - name: lgtm-config
              mountPath: /etc/lgtm
              readOnly: true

            - name: dashboard-files
              mountPath: /otel-lgtm/grafana-dashboard-jvm-metrics.json
              subPath: grafana-homelab-opentelemetry.json

      volumes:
        - name: lgtm-config
          configMap:
            name: otel-lgtm-config

        - name: lgtm-storage
          persistentVolumeClaim:
            claimName: lgtm-observability

        - name: dashboard-files
          configMap:
            name: grafana-dashboards

---
apiVersion: v1
kind: Service
metadata:
  name: otel-lgtm
  namespace: observability
spec:
  selector:
    app: otel-lgtm
  ports:
    - name: grafana
      protocol: TCP
      port: 3000
      targetPort: 3000
    - name: otel-grpc
      protocol: TCP
      port: 4317
      targetPort: 4317
    - name: otel-http
      protocol: TCP
      port: 4318
      targetPort: 4318
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: otel-lgtm-ingress
  namespace: observability
spec:
  ingressClassName: nginx
  rules:
    - host: grafana.homelab
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: otel-lgtm
                port:
                  number: 3000
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: otel-lgtm-tailscale
  namespace: observability
spec:
  ingressClassName: tailscale
  defaultBackend:
    service:
      name: otel-lgtm
      port:
        number: 3000
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-lgtm-config
  namespace: observability
data:
  lgtm-config.yaml: |
    server:
      http_listen_port: 3000
    analytics:
      reporting_enabled: false
    common:
      path_prefix: /var/lgtm
